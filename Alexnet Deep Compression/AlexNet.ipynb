{"cells":[{"cell_type":"markdown","metadata":{"id":"XWJ8ihu5__iA"},"source":["# Tutorial: Compressing AlexNet on Cifar10 dataset achieving `97.83x` compression."]},{"cell_type":"markdown","metadata":{"id":"Tx_OsPAo__iX"},"source":["Compressing the AlexNet neural network on the CIFAR-10 dataset using Condensa. We will target two different objectives: reducing total model memory footprint, and reducing the inference latency of the compressed model.\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import matplotlib\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rBwVmdWzACIP","executionInfo":{"status":"ok","timestamp":1648397286048,"user_tz":-330,"elapsed":5957,"user":{"displayName":"tanmaiyh tentu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdQInquVts565Vl8OY6jYqY3AU2dKIJSedOi0FpA=s64","userId":"18156594695205267971"}},"outputId":"3b625627-b7fc-47ce-e396-d8df136ee99b"},"execution_count":51,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["%cd \"/content/drive/My Drive/notebooks\"\n","!pip install kora -q\n","from kora import drive\n","drive.link_nbs()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qT4MYSVTAL_Y","executionInfo":{"status":"ok","timestamp":1648395565702,"user_tz":-330,"elapsed":10606,"user":{"displayName":"tanmaiyh tentu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdQInquVts565Vl8OY6jYqY3AU2dKIJSedOi0FpA=s64","userId":"18156594695205267971"}},"outputId":"ba27718b-fcdc-46e8-99c2-b9d29a3924b9"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/notebooks\n","\u001b[K     |████████████████████████████████| 57 kB 2.6 MB/s \n","\u001b[K     |████████████████████████████████| 56 kB 3.7 MB/s \n","\u001b[?25h"]}]},{"cell_type":"markdown","metadata":{"id":"YMMT1EED__ie"},"source":["Defining the AlexNet network architecture in PyTorch as shown below:"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"8jOoXrpx__ig","executionInfo":{"status":"ok","timestamp":1648395574776,"user_tz":-330,"elapsed":5973,"user":{"displayName":"tanmaiyh tentu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdQInquVts565Vl8OY6jYqY3AU2dKIJSedOi0FpA=s64","userId":"18156594695205267971"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class AlexNet(nn.Module):\n","    def __init__(self, num_classes=10):\n","        super(AlexNet, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=5),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","        )\n","        self.classifier = nn.Linear(256, num_classes)\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.classifier(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"13URWwc1__ip"},"source":["We instantiate this class into model:\n","\n","\n"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"ZTbNNlC5__is","executionInfo":{"status":"ok","timestamp":1648396437011,"user_tz":-330,"elapsed":630,"user":{"displayName":"tanmaiyh tentu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdQInquVts565Vl8OY6jYqY3AU2dKIJSedOi0FpA=s64","userId":"18156594695205267971"}}},"outputs":[],"source":["model = AlexNet()"]},{"cell_type":"markdown","metadata":{"id":"BI0Y6gqH__iv"},"source":["## Load Pre-Trained Weights"]},{"cell_type":"markdown","metadata":{"id":"FsYKMSNl__iy"},"source":[" Loading a pre-trained set of weights into the model from the `AlexNet.pth` file included with this notebook."]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VLXSu0EH__i1","executionInfo":{"status":"ok","timestamp":1648396439573,"user_tz":-330,"elapsed":6,"user":{"displayName":"tanmaiyh tentu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdQInquVts565Vl8OY6jYqY3AU2dKIJSedOi0FpA=s64","userId":"18156594695205267971"}},"outputId":"232fa80b-f5fd-4e40-ff1b-83320c48d708"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":36}],"source":["model.load_state_dict(torch.load('AlexNet.pth',map_location=torch.device('cpu')))"]},{"cell_type":"markdown","metadata":{"id":"Fgb-SIOF__i3"},"source":["## Preparing for Compression"]},{"cell_type":"markdown","metadata":{"id":"BrYi1gtF__i5"},"source":["Let's make sure CUDA is enabled in PyTorch."]},{"cell_type":"code","execution_count":37,"metadata":{"id":"IVM7Mt1p__i7","executionInfo":{"status":"ok","timestamp":1648396443842,"user_tz":-330,"elapsed":14,"user":{"displayName":"tanmaiyh tentu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdQInquVts565Vl8OY6jYqY3AU2dKIJSedOi0FpA=s64","userId":"18156594695205267971"}}},"outputs":[],"source":["assert torch.cuda.is_available()"]},{"cell_type":"code","source":["!pip install condensa"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oclmtpBnBBLx","executionInfo":{"status":"ok","timestamp":1648396450243,"user_tz":-330,"elapsed":3566,"user":{"displayName":"tanmaiyh tentu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdQInquVts565Vl8OY6jYqY3AU2dKIJSedOi0FpA=s64","userId":"18156594695205267971"}},"outputId":"01ce4e0b-96f6-43eb-84db-d206ce8e4039"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: condensa in /usr/local/lib/python3.7/dist-packages (0.5.0b0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from condensa) (4.63.0)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from condensa) (1.10.0+cu111)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from condensa) (1.21.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->condensa) (3.10.0.2)\n"]}]},{"cell_type":"markdown","metadata":{"id":"Y_QTx1Qq__i9"},"source":["We now create PyTorch data loaders for the training, test, and validation datasets. To save space, we wrap the data loading code into two utility functions: `cifar_train_val_loader` and `cifar_test_loader` (please refer to `util.py` in the current `notebooks` folder for the full code)."]},{"cell_type":"code","execution_count":39,"metadata":{"id":"ynCAd_x5__i_","executionInfo":{"status":"ok","timestamp":1648396451078,"user_tz":-330,"elapsed":14,"user":{"displayName":"tanmaiyh tentu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdQInquVts565Vl8OY6jYqY3AU2dKIJSedOi0FpA=s64","userId":"18156594695205267971"}}},"outputs":[],"source":["import util\n","import torchvision.datasets as datasets"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B9v8P1ld__jB","executionInfo":{"status":"ok","timestamp":1648396460491,"user_tz":-330,"elapsed":7266,"user":{"displayName":"tanmaiyh tentu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdQInquVts565Vl8OY6jYqY3AU2dKIJSedOi0FpA=s64","userId":"18156594695205267971"}},"outputId":"76fe1ee0-736c-4c24-c76a-ce01fd202fce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/condensa/data.py:34: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n","  tensor[i] += torch.from_numpy(nump_array)\n","/usr/local/lib/python3.7/dist-packages/condensa/data.py:34: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n","  tensor[i] += torch.from_numpy(nump_array)\n","/usr/local/lib/python3.7/dist-packages/condensa/data.py:34: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n","  tensor[i] += torch.from_numpy(nump_array)\n","/usr/local/lib/python3.7/dist-packages/condensa/data.py:34: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n","  tensor[i] += torch.from_numpy(nump_array)\n","/usr/local/lib/python3.7/dist-packages/condensa/data.py:34: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n","  tensor[i] += torch.from_numpy(nump_array)\n","/usr/local/lib/python3.7/dist-packages/condensa/data.py:34: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n","  tensor[i] += torch.from_numpy(nump_array)\n","/usr/local/lib/python3.7/dist-packages/condensa/data.py:34: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n","  tensor[i] += torch.from_numpy(nump_array)\n","/usr/local/lib/python3.7/dist-packages/condensa/data.py:34: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n","  tensor[i] += torch.from_numpy(nump_array)\n","/usr/local/lib/python3.7/dist-packages/condensa/data.py:34: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n","  tensor[i] += torch.from_numpy(nump_array)\n","/usr/local/lib/python3.7/dist-packages/condensa/data.py:34: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n","  tensor[i] += torch.from_numpy(nump_array)\n","/usr/local/lib/python3.7/dist-packages/condensa/data.py:34: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n","  tensor[i] += torch.from_numpy(nump_array)\n","/usr/local/lib/python3.7/dist-packages/condensa/data.py:34: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n","  tensor[i] += torch.from_numpy(nump_array)\n","/usr/local/lib/python3.7/dist-packages/condensa/data.py:34: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n","  tensor[i] += torch.from_numpy(nump_array)\n","/usr/local/lib/python3.7/dist-packages/condensa/data.py:34: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n","  tensor[i] += torch.from_numpy(nump_array)\n","/usr/local/lib/python3.7/dist-packages/condensa/data.py:34: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n","  tensor[i] += torch.from_numpy(nump_array)\n","/usr/local/lib/python3.7/dist-packages/condensa/data.py:34: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n","  tensor[i] += torch.from_numpy(nump_array)\n"]},{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/condensa/data.py:34: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n","  tensor[i] += torch.from_numpy(nump_array)\n","/usr/local/lib/python3.7/dist-packages/condensa/data.py:34: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n","  tensor[i] += torch.from_numpy(nump_array)\n","/usr/local/lib/python3.7/dist-packages/condensa/data.py:34: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n","  tensor[i] += torch.from_numpy(nump_array)\n","/usr/local/lib/python3.7/dist-packages/condensa/data.py:34: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n","  tensor[i] += torch.from_numpy(nump_array)\n","/usr/local/lib/python3.7/dist-packages/condensa/data.py:34: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n","  tensor[i] += torch.from_numpy(nump_array)\n","/usr/local/lib/python3.7/dist-packages/condensa/data.py:34: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n","  tensor[i] += torch.from_numpy(nump_array)\n","/usr/local/lib/python3.7/dist-packages/condensa/data.py:34: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n","  tensor[i] += torch.from_numpy(nump_array)\n","/usr/local/lib/python3.7/dist-packages/condensa/data.py:34: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:189.)\n","  tensor[i] += torch.from_numpy(nump_array)\n"]}],"source":["dataset = datasets.CIFAR10\n","\n","trainloader,valloader = util.cifar_train_val_loader(dataset, train_batch_size=128, val_batch_size=128)\n","testloader = util.cifar_test_loader(dataset, batch_size=128)"]},{"cell_type":"markdown","metadata":{"id":"pAQ3mio9__jD"},"source":["The utilities above split the original training set into training and validation sets (using a 9:1 split) and perform data normalization for all datasets. They also utilize Condensa's `GPUDataLoader` to enable fast data prefetching and collation."]},{"cell_type":"markdown","metadata":{"id":"CkCMC0II__jF"},"source":["We now define our loss criterion:"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"KQmlLWLv__jG","executionInfo":{"status":"ok","timestamp":1648397607425,"user_tz":-330,"elapsed":25,"user":{"displayName":"tanmaiyh tentu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdQInquVts565Vl8OY6jYqY3AU2dKIJSedOi0FpA=s64","userId":"18156594695205267971"}}},"outputs":[],"source":["criterion = nn.CrossEntropyLoss().cuda()"]},{"cell_type":"markdown","metadata":{"id":"Jrx1BhJU__jI"},"source":["Finally, we set our logging level to `INFO` so that Condensa prints out intermediate updates."]},{"cell_type":"code","execution_count":56,"metadata":{"id":"UzQc8jwH__jJ","executionInfo":{"status":"ok","timestamp":1648397615398,"user_tz":-330,"elapsed":1966,"user":{"displayName":"tanmaiyh tentu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdQInquVts565Vl8OY6jYqY3AU2dKIJSedOi0FpA=s64","userId":"18156594695205267971"}}},"outputs":[],"source":["import logging\n","logging.basicConfig(level=logging.INFO, format='%(message)s')"]},{"cell_type":"markdown","metadata":{"id":"FW-T38OD__jK"},"source":["## Two Different Compression Strategies"]},{"cell_type":"markdown","metadata":{"id":"qtL4pnuy__jL"},"source":["In this tutorial, we will explore two different ways of compressing the AlexNet network: one targeted at reducing the total model memory footprint (named `MEM`) and the other at reducing inference runtime latency (named `FLOP`)."]},{"cell_type":"markdown","metadata":{"id":"qeHUtVkB__jN"},"source":["### MEM Scheme"]},{"cell_type":"markdown","metadata":{"id":"P0fJXwDa__jO"},"source":["The `MEM` scheme aims to reduce the total model memory footprint (number of bytes required to store the non-zero elements of the compressed model). To this end, we perform a combination of _pruning_ (clipping model parameters to zero) and _quantization_ (using 16-bit floating point representation to store model weights instead of 32-bit). Expressing this scheme in Condensa is fairly straightforward using the built-in [`Compose`](https://nvlabs.github.io/condensa/modules/schemes.html#composition) scheme as shown below:"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"Amucvj4S__jO","executionInfo":{"status":"ok","timestamp":1648397624162,"user_tz":-330,"elapsed":1053,"user":{"displayName":"tanmaiyh tentu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdQInquVts565Vl8OY6jYqY3AU2dKIJSedOi0FpA=s64","userId":"18156594695205267971"}}},"outputs":[],"source":["import condensa\n","from condensa.schemes import Compose, Prune, Quantize\n","\n","MEM = Compose([Prune(0.02), Quantize(condensa.float16)])"]},{"cell_type":"markdown","metadata":{"id":"nhSKq-BE__jQ"},"source":["Here, the operator successively applies pruning followed by quantization to the model. The pruning density, or the ratio of non-zero parameters in the compressed model to the original one, is specified as 0.02 (2%). Condensa includes a number of other common schemes, including structured and block pruning, among others. ) in the API documentation. "]},{"cell_type":"markdown","metadata":{"id":"5M81sEpS__jS"},"source":["### FLOP Scheme"]},{"cell_type":"markdown","metadata":{"id":"1ERYPGoV__jT"},"source":["While the `MEM` scheme is effective at reducing the number of non-zero elements in a model, this may not directly translate into improvements in actual inference runtime. Most modern CPUs and GPUs are unable to detect individual zero elements and bypass computations on them in hardware. Instead, to realize speedups on such architectures, we perform filter pruning, which removes entire filters (3D blocks) at once from convolutional layers. This enables the weight tensors to be physically reshaped in the compressed model. We call this the `FLOP` scheme in this tutorial, and use the [`FilterPrune`](https://nvlabs.github.io/condensa/modules/schemes.html#filter-pruning) scheme in Condensa to define it."]},{"cell_type":"code","execution_count":58,"metadata":{"id":"a7sKzKsP__jU","executionInfo":{"status":"ok","timestamp":1648397629656,"user_tz":-330,"elapsed":586,"user":{"displayName":"tanmaiyh tentu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdQInquVts565Vl8OY6jYqY3AU2dKIJSedOi0FpA=s64","userId":"18156594695205267971"}}},"outputs":[],"source":["from condensa.schemes import FilterPrune\n","FLOP = condensa.schemes.FilterPrune(0.5)"]},{"cell_type":"markdown","metadata":{"id":"UYDiN23P__jV"},"source":["## Setting up the Optimizer"]},{"cell_type":"markdown","metadata":{"id":"xdFchxP___jW"},"source":["To recover any accuracy lost due to compression, Condensa comes with a set of _optimizers_. Each optimizer takes a pre-trained model, applies the compression scheme, and tries to recover the original accuracy either directly or iteratively. In this tutorial, we'll be using Condensa's L-C optimizer. We instantiate it as follows:"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gxlR7id3__jX","executionInfo":{"status":"ok","timestamp":1648397633805,"user_tz":-330,"elapsed":21,"user":{"displayName":"tanmaiyh tentu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjdQInquVts565Vl8OY6jYqY3AU2dKIJSedOi0FpA=s64","userId":"18156594695205267971"}},"outputId":"1682139e-a7ff-428d-d4c6-99d11a429f03"},"outputs":[{"output_type":"stream","name":"stderr","text":["[Condensa] LC ENGINE CONFIG [steps=35, l_optimizer=<class 'condensa.opt.lc.sgd.SGD'>, l_optimizer_params={'momentum': 0.95}, lr=0.01, lr_end=0.0001, lr_decay=None, lr_schedule=None, lr_multiplier=None, mb_iterations_per_l=3000, mb_iterations_first_l=30000, mu_init=0.001, mu_multiplier=1.1, mu_cap=10000, distributed=False, debugging_flags={'custom_model_statistics': <function cnn_statistics at 0x7f44c9603e60>}]\n"]}],"source":["lc = condensa.opt.LC(steps=35,                             # L-C iterations\n","                     l_optimizer=condensa.opt.lc.SGD,      # L-step sub-optimizer\n","                     l_optimizer_params={'momentum':0.95}, # L-step sub-optimizer parameters\n","                     lr=0.01,                              # Initial learning rate\n","                     lr_end=1e-4,                          # Final learning rate\n","                     mb_iterations_per_l=3000,             # Mini-batch iterations per L-step\n","                     mb_iterations_first_l=30000,          # Mini-batch iterations for first L-step\n","                     mu_init=1e-3,                         # Initial value of `mu`\n","                     mu_multiplier=1.1,                    # Multiplier for `mu`\n","                     mu_cap=10000,                         # Maximum value of `mu`\n","                     debugging_flags={'custom_model_statistics':\n","                                      condensa.util.cnn_statistics})\n"]},{"cell_type":"markdown","metadata":{"id":"4FRaIBB7__jY"},"source":["Each optimizer in Condensa has its own set of hyper-parameters which must be specified manually by the user. A full description of hyper-parameter tuning is beyond the scope of this tutorial, but for additional information on what each hyper-parameter represents and tips on finding its optimal value, we refer you to the Condensa paper. In this notebook, we run the L-C algorithm for 35 iterations using the hyper-parameter values shown above. "]},{"cell_type":"markdown","metadata":{"id":"GpeZ4Xa6__jZ"},"source":["## Compressing the Model"]},{"cell_type":"markdown","metadata":{"id":"j5NCFOWo__ja"},"source":["Once the optimizer is instantiated, we can go ahead and perform the actual compression using the [`Compressor`](https://nvlabs.github.io/condensa/modules/compressor.html#model-compressor) class and its [`run`](https://nvlabs.github.io/condensa/modules/compressor.html#condensa.compressor.Compressor.run) method. **Note:** the next two lines may take a while to execute!"]},{"cell_type":"code","source":["compressor_MEM  = condensa.Compressor(lc,\n","                                      MEM,\n","                                      model,\n","                                      trainloader,\n","                                      testloader,\n","                                      valloader,\n","                                      criterion)\n","w_MEM  = compressor_MEM.run()"],"metadata":{"id":"82fKbX7609ML"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["compressor_FLOP = condensa.Compressor(lc,\n","                                      FLOP,\n","                                      model,\n","                                      trainloader,\n","                                      testloader,\n","                                      valloader,\n","                                      criterion)\n","\n","w_FLOP = compressor_FLOP.run()"],"metadata":{"id":"hOgKwozv1BVh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zQlpJ8Pm__jd"},"source":["We specify the optimizer, scheme, input model, training, test, and validation sets, and the loss criterion to create an instance of the Compressor class. Since the optimizer is specified as a parameter, we are able to easily experiment with alternative optimizers in Condensa.\n","\n","In the above snippets, `w_MEM` and `w_FLOP` contain the models compressed using the `MEM` and `FLOP` schemes, respectively. We can now save these to disk:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hm41Xpou__je"},"outputs":[],"source":["torch.save(w_MEM.state_dict(), 'AlexNet_MEM.pth')\n","torch.save(w_FLOP.state_dict(), 'AlexNet_FLOP.pth')"]},{"cell_type":"markdown","metadata":{"id":"fNKG8yRD__jg"},"source":["Condensa also records various statistics about the compression process. These can be retrieved using the `statistics` member of the compressor object as follows:"]},{"cell_type":"code","source":["for k,v in compressor_MEM.statistics.items():\n","    print('{}: {}'.format(k, v))"],"metadata":{"id":"0CokFAWO1J8A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for k,v in compressor_FLOP.statistics.items():\n","    print('{}: {}'.format(k, v))"],"metadata":{"id":"YMakboqu1M0U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iMQFhksD__ji"},"source":["## Results"]},{"cell_type":"markdown","metadata":{"id":"XpO2NW5H__jj"},"source":["We notice that Condensa achieves top-1 test accuracies of **77.49%** and **76.81%** for the MEM and FLOP schemes, respectively (compared to the baseline accuracy of **77.07%** for AlexNet). For more complex models, it is possible to further improve accuracies via model fine-tuning"]},{"cell_type":"markdown","metadata":{"id":"En40MAEG__jj"},"source":["### Compression and Runtime Reductions"]},{"cell_type":"markdown","metadata":{"id":"LCHtHbyH__jk"},"source":["Using the MEM scheme, we reduce the model memory footprint by compressing the neural network by **97.83x**. Additionally, we achieve a **55.6%** reduction in FLOPs using the FLOP scheme."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"AlexNet.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}